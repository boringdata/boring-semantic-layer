{
  "markdown": "# Query Agent: LLM Tool\n\nLLM tools are Python functions that a language model can call during a conversation. When the model needs data, it invokes a tool, receives the result, and continues reasoning.\n\nThe advantage of this approach is that the LLM can directly execute Ibis-style chained queries\u2014unlike MCP, which requires passing JSON payloads through a separate server.\n\n**Benefits:**\n- No additional server to run\n- Full access to native BSL features without an intermediate DSL\n\n## Reference implementation\n\nWe provide a LangChain-based chat agent as a reference:\n\n\ud83d\udc49 [`langchain.py`](https://github.com/boringdata/boring-semantic-layer/blob/main/src/boring_semantic_layer/agents/backends/langchain.py)\n\nThis implementation powers the [BSL CLI demo chat](/agents/chat) and can be adapted to any AI framework (PydanticAI, AI SDK).\n\n## Integrating with your own agent ([LangChain](https://www.langchain.com/))\n\n### Installation\n\nInstall the agent dependency group:\n\n```bash\npip install boring-semantic-layer[agent]\n```\n\nThen install the LLM provider you want to use:\n\n```bash\n# OpenAI\npip install langchain-openai\n\n# Anthropic\npip install langchain-anthropic\n\n# Google\npip install langchain-google-genai\n```\n\n### Usage\n\nThe `LangChainAgent` loads semantic models from a [YAML config file](/building/yaml):\n\n```python\nfrom pathlib import Path\nfrom boring_semantic_layer.agents.backends.langchain import LangChainAgent\n\nagent = LangChainAgent(\n    model_path=Path(\"flights.yaml\"),      # Semantic model YAML\n    llm_model=\"gpt-4o\",                   # LLM to use\n    chart_backend=\"plotext\",              # plotext, altair, or plotly\n    profile=\"dev\",                        # Profile name (optional)\n    profile_file=Path(\"profiles.yml\"),    # Profile file path (optional)\n)\n\ntool_output, response = agent.query(\"What are the top 10 origins by flight count?\")\n```\n\nSee [YAML Config](/building/yaml) for the semantic model format and [Backend Profiles](/building/profile) for connection setup.\n\n## Available tools\n\nThe LLM has access to three tools, similar to the MCP approach:\n\n### `get_documentation`\n\nReturns BSL documentation split into topics (query syntax, methods, charting, etc.). The LLM can explore relevant topics on demand to learn how to construct valid queries and charts.\n\n### `list_models`\n\nLists all available semantic models by name. Useful when multiple models are loaded and the LLM needs to pick the right one.\n\n### `query_model`\n\nExecutes a BSL query and returns results. The LLM passes an Ibis-style query string:\n\n```python\nsm.group_by(\"origin\").aggregate(\"flight_count\")\n```\n\nThe tool executes this query against your semantic model and returns the result.\n\n**Parameters:**\n- `query` \u2014 The BSL query string to execute\n- `show_chart` \u2014 Return a chart visualization\n- `show_table` \u2014 Return a DataFrame\n- `chart_spec` \u2014 Custom Vega-Lite or Plotly chart specification\n- `limit` \u2014 Cap the number of rows returned\n",
  "queries": {},
  "files": {}
}
